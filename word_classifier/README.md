# Распознавание слов с помощью CRNN

Эта папка -- небольшой репозиторий внутри общего. Как понятно из названия, здесь решается задача распознавания слов, т.е. само слово определяется при обработке небольшого изображения, на котором присутствует только это слово. Для этого используется [CRNN](https://arxiv.org/abs/1507.05717) -- нейросеть с последовательно соединенными свёрточными и рекуррентными слоями. С её архитектурой можно более подробно познакомится [в этом jupyter-ноутбуке](crnn_simple_example.ipynb).

Обучение CRNN на датасете из изображений слов выполняется в скрипте `train_crnn.py`, проверка её работы на тестовом датасете -- в скрипте `evaluate_crnn.py`. Для создания такого датасета нужно выполнить несколько шагов:

0. Запустить `shared_paths.py` -- этот скрипт просто создаст все необходимые папки.
1. Собрать аннотированные книги в формате `djvu`, сложить их в папку `data/books` и запустить `book_ids.py`. В результате в папке `data` создастся `csv` файл в котором каждой книге сопоставится некоторый порядковый номер.
2. Запустить `process_books.py` -- этот скрипт извлечёт из книг изображения всех страниц и создаст текстовые файлы со словами и их положением на странице.
3. Запустить `make_word_dataset.py` -- этот скрипт извлечёт из сканов страниц изображения слов и запишет все слова в отдельный текстовый файл. Также там можно отфильтровать слова. Например, сейчас в датасет попадают только слова полностью состоящие из букв кириллицы с 1 знаком препинания в конце, который исключается из слова (но остаётся на картинке).

Этот алгоритм может показаться странным и излишне сложным, потому что таким и является. :) Для получения достаточно большого датасета не потребуется много книг, у меня из 36 файлов в результате было извлечено > 2 млн картинок.

Для обучения модели крайне желательно иметь GPU, причём, наверное, с большим объёмом памяти -- у меня при обучении "съедалось" до 10 ГБ даже при использовании батчей из 64 картинок.

Последний результат на тестовом датасете:
* `CTCLoss = 0.21`
* 93.0% правильно (без единой опечатки) угаданных слов

Также я попробовал использовать метод `MSER` -- maximally stable extremal regions -- для поиска слов на страницах. Пример работы этого алгоритма -- скрипт `detect_words.py`. В принципе, этот метод работает достаточно неплохо, но есть пара недостатков. Во-первых, параметры метода нужно немного варьировать в зависимости от вида страницы, чтобы получить хороший результат. Во-вторых, `MSER` находит много "мусора", соответственно, нужно как-то изменить режим обучения RCNN, чтобы не находить в таких патчах какие-то несуществующие буквы.